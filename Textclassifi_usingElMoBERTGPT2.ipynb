{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text classification using ElMo_BERT_GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idEyIgYm0vPY",
        "colab_type": "text"
      },
      "source": [
        "# Text classification using ElMo, BERT, GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJH1sjyl0od3",
        "colab_type": "text"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wFyj25qUiMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "np.random.seed(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPfAFCyY04C5",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh3gU1Y7VUuB",
        "colab_type": "code",
        "outputId": "cecb3067-8877-440b-ac7b-2cec0b6654b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install git+https://github.com/zalandoresearch/flair.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/zalandoresearch/flair.git\n",
            "  Cloning https://github.com/zalandoresearch/flair.git to /tmp/pip-req-build-c5fux2nl\n",
            "  Running command git clone -q https://github.com/zalandoresearch/flair.git /tmp/pip-req-build-c5fux2nl\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): flair==0.4.3 from git+https://github.com/zalandoresearch/flair.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: pytorch-transformers>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (1.2.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (1.1.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (1.0.7)\n",
            "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (0.3.0)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (4.28.1)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (0.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (0.0)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (0.1.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (1.24.3)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (1.2.6)\n",
            "Requirement already satisfied: ipython==7.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (7.6.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (2019.8.19)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (1.5.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (0.3.0)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (1.6.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (3.0.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (0.8.3)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (3.6.0)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (3.6.4)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.4.3) (0.2.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers>=1.1.0->flair==0.4.3) (1.9.224)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers>=1.1.0->flair==0.4.3) (0.0.34)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers>=1.1.0->flair==0.4.3) (1.16.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers>=1.1.0->flair==0.4.3) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers>=1.1.0->flair==0.4.3) (0.1.83)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair==0.4.3) (1.12.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair==0.4.3) (0.21.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.3) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.3) (1.3.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.3) (2.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.4.3) (3.9.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.4.3) (1.11.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair==0.4.3) (2.1.3)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair==0.4.3) (0.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair==0.4.3) (2.0.9)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair==0.4.3) (0.15.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair==0.4.3) (4.7.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair==0.4.3) (41.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair==0.4.3) (4.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair==0.4.3) (4.3.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair==0.4.3) (0.7.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair==0.4.3) (4.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.3) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.3) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.4.3) (2.4.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair==0.4.3) (1.8.4)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.3) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.3) (1.8.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.3) (19.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.3) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair==0.4.3) (7.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers>=1.1.0->flair==0.4.3) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers>=1.1.0->flair==0.4.3) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers>=1.1.0->flair==0.4.3) (1.12.224)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers>=1.1.0->flair==0.4.3) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers>=1.1.0->flair==0.4.3) (0.13.2)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers>=1.1.0->flair==0.4.3) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers>=1.1.0->flair==0.4.3) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers>=1.1.0->flair==0.4.3) (3.0.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair==0.4.3) (0.1.7)\n",
            "Requirement already satisfied: parso>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair==0.4.3) (0.5.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair==0.4.3) (0.6.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->flair==0.4.3) (0.46)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair==0.4.3) (2.49.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch-transformers>=1.1.0->flair==0.4.3) (0.15.2)\n",
            "Building wheels for collected packages: flair\n",
            "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.4.3-cp36-none-any.whl size=113131 sha256=288fe9da66cea325a2dfcc2fa33a347ca1eead0ba717f360e3346dc7f12de2e1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gc8eebxq/wheels/6a/78/0f/399330241d3bc69458cc4fe320dcdfbf818f9887803f0294e7\n",
            "Successfully built flair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX4BTdpbVjK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFCzOGKxVp-e",
        "colab_type": "code",
        "outputId": "41a71d9f-bff2-4560-b88d-5b92e54a9a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from flair.embeddings import BertEmbeddings,ELMoEmbeddings,OpenAIGPTEmbeddings\n",
        "\n",
        "# init embedding\n",
        "embed = BertEmbeddings()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1217314.09B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 78064.88B/s]\n",
            "100%|██████████| 440473133/440473133 [00:12<00:00, 34378837.40B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "94d1eb47-3266-4ddb-e589-96ac1f109794",
        "id": "rYxQztyiAC3K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "sentence = Sentence('Flair is pretty neat!')\n",
        "classifier.predict(sentence)\n",
        "# print sentence with predicted labels\n",
        "print('Sentence above is: ', sentence.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:43:34,037 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/classy-imdb-en-rnn-cuda%3A0/imdb-v0.4.pt not found in cache, downloading to /tmp/tmp1emr07fz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501979561/1501979561 [01:28<00:00, 17015115.62B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:45:02,986 copying /tmp/tmp1emr07fz to cache at /root/.flair/models/imdb-v0.4.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:45:10,784 removing temp file /tmp/tmp1emr07fz\n",
            "2019-09-25 02:45:10,971 loading file /root/.flair/models/imdb-v0.4.pt\n",
            "Sentence above is:  [POSITIVE (0.6636107563972473)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6878f198-ad75-4b31-b03f-473b17a5f59a",
        "id": "Gi2Z0mdEACDO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  test_data.txt  train_5500.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpdFlvTM2jis",
        "colab_type": "text"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py5FUAQtVpwE",
        "colab_type": "code",
        "outputId": "ce32204a-9db1-46de-b666-50c3f9e617b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Tony607/Keras-Text-Transfer-Learning/master/train_5500.txt\n",
        "!wget https://raw.githubusercontent.com/Tony607/Keras-Text-Transfer-Learning/master/test_data.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-25 02:45:29--  https://raw.githubusercontent.com/Tony607/Keras-Text-Transfer-Learning/master/train_5500.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 335860 (328K) [text/plain]\n",
            "Saving to: ‘train_5500.txt’\n",
            "\n",
            "\rtrain_5500.txt        0%[                    ]       0  --.-KB/s               \rtrain_5500.txt      100%[===================>] 327.99K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-09-25 02:45:29 (11.3 MB/s) - ‘train_5500.txt’ saved [335860/335860]\n",
            "\n",
            "--2019-09-25 02:45:31--  https://raw.githubusercontent.com/Tony607/Keras-Text-Transfer-Learning/master/test_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23354 (23K) [text/plain]\n",
            "Saving to: ‘test_data.txt’\n",
            "\n",
            "test_data.txt       100%[===================>]  22.81K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2019-09-25 02:45:31 (3.17 MB/s) - ‘test_data.txt’ saved [23354/23354]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2eIRUj3X-3",
        "colab_type": "text"
      },
      "source": [
        "## Decscription of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXpR6dEE3a6r",
        "colab_type": "text"
      },
      "source": [
        "The dataset we use is the TREC Question Classification dataset, There are entirely 5452 training and 500 test samples, that is 5452 + 500 questions each categorized into one of the six labels.\n",
        "\n",
        "- ABBR - 'abbreviation': expression abbreviated, etc.\n",
        "- DESC - 'description and abstract concepts': manner of an action, description of sth. etc.\n",
        "- ENTY - 'entities': animals, colors, events, food, etc.\n",
        "- HUM - 'human beings': a group or organization of persons, an individual, etc.\n",
        "- LOC - 'locations': cities, countries, etc.\n",
        "- NUM - 'numeric values': postcodes, dates, speed,temperature, etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6NTBzTaV3a9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract lines from .txt and convert to dataframe\n",
        "\n",
        "def get_dataframe(filename):\n",
        "    lines = open(filename, 'r').read().splitlines()\n",
        "    data = []\n",
        "    for i in range(0, len(lines)):\n",
        "        label = lines[i].split(' ')[0]\n",
        "        label = label.split(\":\")[0]\n",
        "        text = ' '.join(lines[i].split(' ')[1:])\n",
        "        text = re.sub('[^A-Za-z0-9 ,\\?\\'\\\"-._\\+\\!/\\`@=;:]+', '', text)\n",
        "        data.append([label, text])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['label', 'text'])\n",
        "    df.label = df.label.astype('category')\n",
        "    return df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KFQSdpSWEB7",
        "colab_type": "code",
        "outputId": "c9a2ea34-6406-4ee9-dfb0-0f1be76b3c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#Assign train data\n",
        "df_train = get_dataframe('train_5500.txt')\n",
        "print(df_train.head())\n",
        "df_train.to_csv('train.csv')\n",
        "df_test = get_dataframe('test_data.txt')\n",
        "print(df_test.head())\n",
        "df_test.to_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  label                                               text\n",
            "0  DESC  How did serfdom develop in and then leave Russ...\n",
            "1  ENTY   What films featured the character Popeye Doyle ?\n",
            "2  DESC  How can I find a list of celebrities ' real na...\n",
            "3  ENTY  What fowl grabs the spotlight after the Chines...\n",
            "4  ABBR                    What is the full form of .com ?\n",
            "  label                                      text\n",
            "0   NUM      How far is it from Denver to Aspen ?\n",
            "1   LOC  What county is Modesto , California in ?\n",
            "2   HUM                         Who was Galileo ?\n",
            "3  DESC                         What is an atom ?\n",
            "4   NUM          When did Hawaii become a state ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTwA-WJbWIC0",
        "colab_type": "code",
        "outputId": "82d7a0b1-d9ec-432c-da97-7c491d429f7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Number of categories in dataset\n",
        "category_counts = len(df_train.label.cat.categories)\n",
        "category_counts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyPHxfboAZjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"./train.csv\", encoding='latin-1').sample(frac=1).drop_duplicates()\n",
        "data = data[['label', 'text']].rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
        " \n",
        "data['label'] = '__label__' + data['label'].astype(str)\n",
        "\n",
        "data.iloc[0:int(len(data)*0.8)].to_csv('train.csv', sep='\\t', index = False, header = False, columns=['label', 'text'])\n",
        "data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv('test.csv', sep='\\t', index = False, header = False, columns=['label', 'text'])\n",
        "data.iloc[int(len(data)*0.9):].to_csv('dev.csv', sep='\\t', index = False, header = False, columns=['label', 'text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zmnSgOgZZmv",
        "colab_type": "code",
        "outputId": "02376988-23b1-4c28-bed7-1d05df16bb34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings, BertEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
        "word_embeddings = [WordEmbeddings('glove'), BertEmbeddings('bert-base-uncased'), FlairEmbeddings('news-backward-fast')]\n",
        "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
        "\n",
        "\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
        "\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "trainer.train('./', max_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:45:39,245 Reading data from .\n",
            "2019-09-25 02:45:39,247 Train: train.csv\n",
            "2019-09-25 02:45:39,248 Dev: dev.csv\n",
            "2019-09-25 02:45:39,250 Test: test.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:45:40,794 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmp7bd8xn68\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 160000128/160000128 [00:10<00:00, 14875900.75B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:45:52,265 copying /tmp/tmp7bd8xn68 to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:45:52,677 removing temp file /tmp/tmp7bd8xn68\n",
            "2019-09-25 02:45:53,312 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /tmp/tmpnqy5yd5v\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21494764/21494764 [00:02<00:00, 8721699.00B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:45:56,445 copying /tmp/tmpnqy5yd5v to cache at /root/.flair/embeddings/glove.gensim\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:45:56,476 removing temp file /tmp/tmpnqy5yd5v\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:46:02,893 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt not found in cache, downloading to /tmp/tmp150ghs0u\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19689779/19689779 [00:02<00:00, 7946637.99B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:46:06,035 copying /tmp/tmp150ghs0u to cache at /root/.flair/embeddings/lm-news-english-backward-1024-v0.2rc.pt\n",
            "2019-09-25 02:46:06,060 removing temp file /tmp/tmp150ghs0u\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:46:06,873 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "100%|██████████| 4361/4361 [00:00<00:00, 234748.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:46:06,898 [b'HUM', b'LOC', b'NUM', b'ABBR', b'ENTY', b'DESC']\n",
            "2019-09-25 02:46:06,914 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:46:06,919 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentLSTMEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): WordEmbeddings('glove')\n",
            "      (list_embedding_1): BertEmbeddings(\n",
            "        (model): BertModel(\n",
            "          (embeddings): BertEmbeddings(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1)\n",
            "          )\n",
            "          (encoder): BertEncoder(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayer(\n",
            "                (attention): BertAttention(\n",
            "                  (self): BertSelfAttention(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (list_embedding_2): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.25)\n",
            "          (encoder): Embedding(275, 100)\n",
            "          (rnn): LSTM(100, 1024)\n",
            "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
            "    (rnn): GRU(256, 512)\n",
            "    (dropout): Dropout(p=0.5)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=6, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            ")\"\n",
            "2019-09-25 02:46:06,921 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:46:06,923 Corpus: \"Corpus: 4361 train + 546 dev + 545 test sentences\"\n",
            "2019-09-25 02:46:06,924 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:46:06,925 Parameters:\n",
            "2019-09-25 02:46:06,927  - learning_rate: \"0.1\"\n",
            "2019-09-25 02:46:06,928  - mini_batch_size: \"32\"\n",
            "2019-09-25 02:46:06,929  - patience: \"3\"\n",
            "2019-09-25 02:46:06,931  - anneal_factor: \"0.5\"\n",
            "2019-09-25 02:46:06,932  - max_epochs: \"10\"\n",
            "2019-09-25 02:46:06,933  - shuffle: \"True\"\n",
            "2019-09-25 02:46:06,934  - train_with_dev: \"False\"\n",
            "2019-09-25 02:46:06,936  - batch_growth_annealing: \"False\"\n",
            "2019-09-25 02:46:06,937 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:46:06,938 Model training base path: \".\"\n",
            "2019-09-25 02:46:06,939 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:46:06,941 Device: cpu\n",
            "2019-09-25 02:46:06,942 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:46:06,943 Embeddings storage mode: cpu\n",
            "2019-09-25 02:46:06,947 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 02:46:10,812 epoch 1 - iter 0/137 - loss 1.80171144 - samples/sec: 107.74\n",
            "2019-09-25 02:47:01,253 epoch 1 - iter 13/137 - loss 2.26821761 - samples/sec: 8.27\n",
            "2019-09-25 02:47:53,379 epoch 1 - iter 26/137 - loss 2.10939177 - samples/sec: 8.00\n",
            "2019-09-25 02:48:45,516 epoch 1 - iter 39/137 - loss 1.99575513 - samples/sec: 7.99\n",
            "2019-09-25 02:49:36,032 epoch 1 - iter 52/137 - loss 1.94803076 - samples/sec: 8.25\n",
            "2019-09-25 02:50:28,462 epoch 1 - iter 65/137 - loss 1.83922567 - samples/sec: 7.95\n",
            "2019-09-25 02:51:16,016 epoch 1 - iter 78/137 - loss 1.80030416 - samples/sec: 8.77\n",
            "2019-09-25 02:52:08,141 epoch 1 - iter 91/137 - loss 1.72820347 - samples/sec: 8.00\n",
            "2019-09-25 02:53:00,244 epoch 1 - iter 104/137 - loss 1.69269149 - samples/sec: 8.00\n",
            "2019-09-25 02:53:50,726 epoch 1 - iter 117/137 - loss 1.64486988 - samples/sec: 8.26\n",
            "2019-09-25 02:54:41,693 epoch 1 - iter 130/137 - loss 1.59708728 - samples/sec: 8.18\n",
            "2019-09-25 02:55:02,360 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:55:02,362 EPOCH 1 done: loss 1.5839 - lr 0.1000\n",
            "2019-09-25 02:56:05,345 DEV : loss 0.8732489943504333 - score 0.6319\n",
            "2019-09-25 02:56:05,378 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 02:56:10,718 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:56:11,330 epoch 2 - iter 0/137 - loss 0.96427286 - samples/sec: 686.59\n",
            "2019-09-25 02:56:15,349 epoch 2 - iter 13/137 - loss 1.10880591 - samples/sec: 107.54\n",
            "2019-09-25 02:56:19,447 epoch 2 - iter 26/137 - loss 1.10954465 - samples/sec: 104.20\n",
            "2019-09-25 02:56:23,236 epoch 2 - iter 39/137 - loss 1.06792606 - samples/sec: 112.92\n",
            "2019-09-25 02:56:27,347 epoch 2 - iter 52/137 - loss 1.02438848 - samples/sec: 103.93\n",
            "2019-09-25 02:56:31,012 epoch 2 - iter 65/137 - loss 1.00929795 - samples/sec: 116.79\n",
            "2019-09-25 02:56:34,732 epoch 2 - iter 78/137 - loss 0.99336256 - samples/sec: 115.05\n",
            "2019-09-25 02:56:38,302 epoch 2 - iter 91/137 - loss 0.98692304 - samples/sec: 119.92\n",
            "2019-09-25 02:56:41,686 epoch 2 - iter 104/137 - loss 0.97670924 - samples/sec: 126.70\n",
            "2019-09-25 02:56:45,067 epoch 2 - iter 117/137 - loss 0.95887434 - samples/sec: 127.19\n",
            "2019-09-25 02:56:48,951 epoch 2 - iter 130/137 - loss 0.96372854 - samples/sec: 110.04\n",
            "2019-09-25 02:56:50,439 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:56:50,441 EPOCH 2 done: loss 0.9675 - lr 0.1000\n",
            "2019-09-25 02:56:51,601 DEV : loss 0.7113815546035767 - score 0.7125\n",
            "2019-09-25 02:56:51,635 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 02:56:56,794 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:56:57,044 epoch 3 - iter 0/137 - loss 0.85295743 - samples/sec: 1725.55\n",
            "2019-09-25 02:57:00,696 epoch 3 - iter 13/137 - loss 0.63610855 - samples/sec: 117.24\n",
            "2019-09-25 02:57:04,118 epoch 3 - iter 26/137 - loss 0.78997100 - samples/sec: 125.71\n",
            "2019-09-25 02:57:07,627 epoch 3 - iter 39/137 - loss 0.80758950 - samples/sec: 122.10\n",
            "2019-09-25 02:57:11,072 epoch 3 - iter 52/137 - loss 0.78224375 - samples/sec: 124.42\n",
            "2019-09-25 02:57:14,487 epoch 3 - iter 65/137 - loss 0.77967736 - samples/sec: 125.63\n",
            "2019-09-25 02:57:17,920 epoch 3 - iter 78/137 - loss 0.79762343 - samples/sec: 124.91\n",
            "2019-09-25 02:57:21,781 epoch 3 - iter 91/137 - loss 0.78816767 - samples/sec: 110.73\n",
            "2019-09-25 02:57:25,612 epoch 3 - iter 104/137 - loss 0.77949347 - samples/sec: 111.63\n",
            "2019-09-25 02:57:29,726 epoch 3 - iter 117/137 - loss 0.79087566 - samples/sec: 103.98\n",
            "2019-09-25 02:57:33,824 epoch 3 - iter 130/137 - loss 0.78307252 - samples/sec: 104.26\n",
            "2019-09-25 02:57:35,506 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:57:35,508 EPOCH 3 done: loss 0.7820 - lr 0.1000\n",
            "2019-09-25 02:57:36,762 DEV : loss 0.7527585625648499 - score 0.7143\n",
            "2019-09-25 02:57:36,799 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 02:57:41,987 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:57:42,278 epoch 4 - iter 0/137 - loss 0.74819654 - samples/sec: 1459.00\n",
            "2019-09-25 02:57:46,131 epoch 4 - iter 13/137 - loss 0.76334308 - samples/sec: 111.32\n",
            "2019-09-25 02:57:49,880 epoch 4 - iter 26/137 - loss 0.68778001 - samples/sec: 114.40\n",
            "2019-09-25 02:57:53,172 epoch 4 - iter 39/137 - loss 0.66568751 - samples/sec: 130.40\n",
            "2019-09-25 02:57:56,437 epoch 4 - iter 52/137 - loss 0.67759600 - samples/sec: 131.77\n",
            "2019-09-25 02:58:00,036 epoch 4 - iter 65/137 - loss 0.64726021 - samples/sec: 119.08\n",
            "2019-09-25 02:58:03,431 epoch 4 - iter 78/137 - loss 0.64187920 - samples/sec: 126.62\n",
            "2019-09-25 02:58:07,078 epoch 4 - iter 91/137 - loss 0.64042494 - samples/sec: 117.42\n",
            "2019-09-25 02:58:10,831 epoch 4 - iter 104/137 - loss 0.64443861 - samples/sec: 114.13\n",
            "2019-09-25 02:58:14,364 epoch 4 - iter 117/137 - loss 0.65138895 - samples/sec: 121.31\n",
            "2019-09-25 02:58:17,959 epoch 4 - iter 130/137 - loss 0.63956140 - samples/sec: 119.29\n",
            "2019-09-25 02:58:19,730 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:58:19,732 EPOCH 4 done: loss 0.6385 - lr 0.1000\n",
            "2019-09-25 02:58:20,962 DEV : loss 1.0911016464233398 - score 0.6447\n",
            "2019-09-25 02:58:20,997 BAD EPOCHS (no improvement): 1\n",
            "2019-09-25 02:58:20,999 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:58:21,293 epoch 5 - iter 0/137 - loss 0.77705950 - samples/sec: 1430.57\n",
            "2019-09-25 02:58:24,781 epoch 5 - iter 13/137 - loss 0.51596665 - samples/sec: 123.27\n",
            "2019-09-25 02:58:28,446 epoch 5 - iter 26/137 - loss 0.52640375 - samples/sec: 117.16\n",
            "2019-09-25 02:58:32,331 epoch 5 - iter 39/137 - loss 0.57707746 - samples/sec: 110.14\n",
            "2019-09-25 02:58:36,168 epoch 5 - iter 52/137 - loss 0.56386091 - samples/sec: 111.47\n",
            "2019-09-25 02:58:39,741 epoch 5 - iter 65/137 - loss 0.55682120 - samples/sec: 119.86\n",
            "2019-09-25 02:58:43,364 epoch 5 - iter 78/137 - loss 0.54425515 - samples/sec: 118.13\n",
            "2019-09-25 02:58:46,846 epoch 5 - iter 91/137 - loss 0.54385242 - samples/sec: 123.16\n",
            "2019-09-25 02:58:50,451 epoch 5 - iter 104/137 - loss 0.53938197 - samples/sec: 118.76\n",
            "2019-09-25 02:58:54,411 epoch 5 - iter 117/137 - loss 0.54002819 - samples/sec: 107.97\n",
            "2019-09-25 02:58:58,764 epoch 5 - iter 130/137 - loss 0.53738400 - samples/sec: 98.05\n",
            "2019-09-25 02:59:00,541 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:59:00,542 EPOCH 5 done: loss 0.5310 - lr 0.1000\n",
            "2019-09-25 02:59:01,734 DEV : loss 0.4972025454044342 - score 0.7967\n",
            "2019-09-25 02:59:01,767 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 02:59:06,822 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:59:07,166 epoch 6 - iter 0/137 - loss 0.43706816 - samples/sec: 1228.41\n",
            "2019-09-25 02:59:10,467 epoch 6 - iter 13/137 - loss 0.49218871 - samples/sec: 130.50\n",
            "2019-09-25 02:59:13,910 epoch 6 - iter 26/137 - loss 0.45717162 - samples/sec: 124.71\n",
            "2019-09-25 02:59:17,221 epoch 6 - iter 39/137 - loss 0.44019458 - samples/sec: 129.68\n",
            "2019-09-25 02:59:20,747 epoch 6 - iter 52/137 - loss 0.46583919 - samples/sec: 121.66\n",
            "2019-09-25 02:59:24,000 epoch 6 - iter 65/137 - loss 0.45563889 - samples/sec: 132.33\n",
            "2019-09-25 02:59:27,609 epoch 6 - iter 78/137 - loss 0.47220305 - samples/sec: 119.07\n",
            "2019-09-25 02:59:31,120 epoch 6 - iter 91/137 - loss 0.48013254 - samples/sec: 122.26\n",
            "2019-09-25 02:59:34,460 epoch 6 - iter 104/137 - loss 0.46896628 - samples/sec: 128.86\n",
            "2019-09-25 02:59:37,994 epoch 6 - iter 117/137 - loss 0.46258680 - samples/sec: 121.31\n",
            "2019-09-25 02:59:42,374 epoch 6 - iter 130/137 - loss 0.44821543 - samples/sec: 97.31\n",
            "2019-09-25 02:59:44,362 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:59:44,364 EPOCH 6 done: loss 0.4373 - lr 0.1000\n",
            "2019-09-25 02:59:45,693 DEV : loss 0.4474552869796753 - score 0.8462\n",
            "2019-09-25 02:59:45,732 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 02:59:50,647 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 02:59:51,041 epoch 7 - iter 0/137 - loss 0.46182466 - samples/sec: 1065.50\n",
            "2019-09-25 02:59:54,643 epoch 7 - iter 13/137 - loss 0.55556880 - samples/sec: 119.26\n",
            "2019-09-25 02:59:58,021 epoch 7 - iter 26/137 - loss 0.51921527 - samples/sec: 127.39\n",
            "2019-09-25 03:00:01,817 epoch 7 - iter 39/137 - loss 0.45217469 - samples/sec: 112.70\n",
            "2019-09-25 03:00:05,379 epoch 7 - iter 52/137 - loss 0.42451368 - samples/sec: 120.70\n",
            "2019-09-25 03:00:09,134 epoch 7 - iter 65/137 - loss 0.40703442 - samples/sec: 114.03\n",
            "2019-09-25 03:00:12,623 epoch 7 - iter 78/137 - loss 0.38906880 - samples/sec: 123.08\n",
            "2019-09-25 03:00:16,156 epoch 7 - iter 91/137 - loss 0.39511889 - samples/sec: 121.61\n",
            "2019-09-25 03:00:19,755 epoch 7 - iter 104/137 - loss 0.38783678 - samples/sec: 119.08\n",
            "2019-09-25 03:00:23,253 epoch 7 - iter 117/137 - loss 0.38758087 - samples/sec: 122.60\n",
            "2019-09-25 03:00:27,466 epoch 7 - iter 130/137 - loss 0.38161137 - samples/sec: 101.38\n",
            "2019-09-25 03:00:29,141 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:00:29,142 EPOCH 7 done: loss 0.3741 - lr 0.1000\n",
            "2019-09-25 03:00:30,330 DEV : loss 0.44733327627182007 - score 0.8626\n",
            "2019-09-25 03:00:30,363 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 03:00:35,637 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:00:35,956 epoch 8 - iter 0/137 - loss 0.28821152 - samples/sec: 1333.94\n",
            "2019-09-25 03:00:39,580 epoch 8 - iter 13/137 - loss 0.26020097 - samples/sec: 118.33\n",
            "2019-09-25 03:00:43,053 epoch 8 - iter 26/137 - loss 0.25165561 - samples/sec: 124.03\n",
            "2019-09-25 03:00:46,642 epoch 8 - iter 39/137 - loss 0.27847615 - samples/sec: 119.65\n",
            "2019-09-25 03:00:50,489 epoch 8 - iter 52/137 - loss 0.27875211 - samples/sec: 111.21\n",
            "2019-09-25 03:00:54,369 epoch 8 - iter 65/137 - loss 0.28689377 - samples/sec: 110.41\n",
            "2019-09-25 03:00:57,979 epoch 8 - iter 78/137 - loss 0.28738612 - samples/sec: 118.70\n",
            "2019-09-25 03:01:01,579 epoch 8 - iter 91/137 - loss 0.27876904 - samples/sec: 119.16\n",
            "2019-09-25 03:01:05,378 epoch 8 - iter 104/137 - loss 0.28105000 - samples/sec: 112.67\n",
            "2019-09-25 03:01:08,921 epoch 8 - iter 117/137 - loss 0.28630670 - samples/sec: 121.13\n",
            "2019-09-25 03:01:12,870 epoch 8 - iter 130/137 - loss 0.28528580 - samples/sec: 108.31\n",
            "2019-09-25 03:01:14,803 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:01:14,805 EPOCH 8 done: loss 0.2847 - lr 0.1000\n",
            "2019-09-25 03:01:16,011 DEV : loss 0.28305742144584656 - score 0.9066\n",
            "2019-09-25 03:01:16,045 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 03:01:21,075 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:01:21,431 epoch 9 - iter 0/137 - loss 0.28533226 - samples/sec: 1182.52\n",
            "2019-09-25 03:01:25,198 epoch 9 - iter 13/137 - loss 0.23533695 - samples/sec: 114.00\n",
            "2019-09-25 03:01:28,980 epoch 9 - iter 26/137 - loss 0.22519103 - samples/sec: 113.16\n",
            "2019-09-25 03:01:32,904 epoch 9 - iter 39/137 - loss 0.23306834 - samples/sec: 108.96\n",
            "2019-09-25 03:01:36,845 epoch 9 - iter 52/137 - loss 0.23188673 - samples/sec: 108.57\n",
            "2019-09-25 03:01:40,957 epoch 9 - iter 65/137 - loss 0.24101272 - samples/sec: 103.81\n",
            "2019-09-25 03:01:44,935 epoch 9 - iter 78/137 - loss 0.24983759 - samples/sec: 107.41\n",
            "2019-09-25 03:01:48,688 epoch 9 - iter 91/137 - loss 0.25357246 - samples/sec: 114.09\n",
            "2019-09-25 03:01:52,313 epoch 9 - iter 104/137 - loss 0.24828613 - samples/sec: 118.14\n",
            "2019-09-25 03:01:56,182 epoch 9 - iter 117/137 - loss 0.24772457 - samples/sec: 110.66\n",
            "2019-09-25 03:01:59,984 epoch 9 - iter 130/137 - loss 0.24983394 - samples/sec: 112.51\n",
            "2019-09-25 03:02:01,494 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:02:01,496 EPOCH 9 done: loss 0.2473 - lr 0.1000\n",
            "2019-09-25 03:02:02,725 DEV : loss 0.2592355012893677 - score 0.8974\n",
            "2019-09-25 03:02:02,761 BAD EPOCHS (no improvement): 1\n",
            "2019-09-25 03:02:02,794 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:02:03,211 epoch 10 - iter 0/137 - loss 0.03594280 - samples/sec: 1012.64\n",
            "2019-09-25 03:02:07,036 epoch 10 - iter 13/137 - loss 0.24543898 - samples/sec: 111.84\n",
            "2019-09-25 03:02:10,799 epoch 10 - iter 26/137 - loss 0.21815453 - samples/sec: 113.74\n",
            "2019-09-25 03:02:14,261 epoch 10 - iter 39/137 - loss 0.20983434 - samples/sec: 124.24\n",
            "2019-09-25 03:02:17,935 epoch 10 - iter 52/137 - loss 0.20940754 - samples/sec: 116.38\n",
            "2019-09-25 03:02:22,002 epoch 10 - iter 65/137 - loss 0.20129409 - samples/sec: 105.28\n",
            "2019-09-25 03:02:25,769 epoch 10 - iter 78/137 - loss 0.19824184 - samples/sec: 113.93\n",
            "2019-09-25 03:02:29,349 epoch 10 - iter 91/137 - loss 0.20812761 - samples/sec: 119.87\n",
            "2019-09-25 03:02:33,253 epoch 10 - iter 104/137 - loss 0.21083744 - samples/sec: 109.48\n",
            "2019-09-25 03:02:37,324 epoch 10 - iter 117/137 - loss 0.20847462 - samples/sec: 105.05\n",
            "2019-09-25 03:02:41,346 epoch 10 - iter 130/137 - loss 0.20970622 - samples/sec: 106.18\n",
            "2019-09-25 03:02:43,183 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:02:43,184 EPOCH 10 done: loss 0.2070 - lr 0.1000\n",
            "2019-09-25 03:02:44,461 DEV : loss 0.24229063093662262 - score 0.9139\n",
            "2019-09-25 03:02:44,498 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 03:02:54,907 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:02:54,908 Testing using best model ...\n",
            "2019-09-25 03:02:54,917 loading file best-model.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:574: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  result = unpickler.load()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:04:04,235 0.9138\t0.9138\t0.9138\n",
            "2019-09-25 03:04:04,236 \n",
            "MICRO_AVG: acc 0.8412 - f1-score 0.9138\n",
            "MACRO_AVG: acc 0.7806 - f1-score 0.8656833333333332\n",
            "ABBR       tp: 4 - fp: 1 - fn: 4 - tn: 536 - precision: 0.8000 - recall: 0.5000 - accuracy: 0.4444 - f1-score: 0.6154\n",
            "DESC       tp: 98 - fp: 12 - fn: 16 - tn: 419 - precision: 0.8909 - recall: 0.8596 - accuracy: 0.7778 - f1-score: 0.8750\n",
            "ENTY       tp: 86 - fp: 21 - fn: 9 - tn: 429 - precision: 0.8037 - recall: 0.9053 - accuracy: 0.7414 - f1-score: 0.8515\n",
            "HUM        tp: 121 - fp: 3 - fn: 7 - tn: 414 - precision: 0.9758 - recall: 0.9453 - accuracy: 0.9237 - f1-score: 0.9603\n",
            "LOC        tp: 86 - fp: 3 - fn: 11 - tn: 445 - precision: 0.9663 - recall: 0.8866 - accuracy: 0.8600 - f1-score: 0.9247\n",
            "NUM        tp: 103 - fp: 7 - fn: 0 - tn: 435 - precision: 0.9364 - recall: 1.0000 - accuracy: 0.9364 - f1-score: 0.9672\n",
            "2019-09-25 03:04:04,241 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.9138,\n",
              " 'dev_score_history': [0.6319,\n",
              "  0.7125,\n",
              "  0.7143,\n",
              "  0.6447,\n",
              "  0.7967,\n",
              "  0.8462,\n",
              "  0.8626,\n",
              "  0.9066,\n",
              "  0.8974,\n",
              "  0.9139],\n",
              " 'train_loss_history': [1.583923603061342,\n",
              "  0.9675393302510255,\n",
              "  0.7819692168357598,\n",
              "  0.6384952681778121,\n",
              "  0.5309577492901879,\n",
              "  0.43726150593618407,\n",
              "  0.37412173333611803,\n",
              "  0.2846662643073249,\n",
              "  0.24728865875271114,\n",
              "  0.2069828187244652],\n",
              " 'dev_loss_history': [tensor(0.8732),\n",
              "  tensor(0.7114),\n",
              "  tensor(0.7528),\n",
              "  tensor(1.0911),\n",
              "  tensor(0.4972),\n",
              "  tensor(0.4475),\n",
              "  tensor(0.4473),\n",
              "  tensor(0.2831),\n",
              "  tensor(0.2592),\n",
              "  tensor(0.2423)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIobYwCuMFq2",
        "colab_type": "code",
        "outputId": "964e1416-9bf8-495a-9155-d8c05163adb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "classifier = TextClassifier.load('./best-model.pt')\n",
        "sentence = Sentence(\"What is the full form of .col?\")\n",
        "classifier.predict(sentence)\n",
        "print(sentence.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:05:15,188 loading file ./best-model.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:574: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  result = unpickler.load()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ABBR (0.5854447484016418)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kopr88fcOYe0",
        "colab_type": "code",
        "outputId": "3c8a5b46-eef7-4cf9-f767-ba3d5aa3824f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/bc/e30325523363215c503171822f09436adcfbc74f426ad62496276f1ac4c0/allennlp-0.8.5-py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.3)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 44.0MB/s \n",
            "\u001b[?25hCollecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Collecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
            "Collecting word2number>=1.1 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting conllu==1.3.1 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting flask-cors>=3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 22.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Collecting parsimonious>=0.8.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 21.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting jsonpickle (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch<1.2,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Collecting numpydoc>=0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 42.1MB/s \n",
            "\u001b[?25hCollecting unidecode (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.224)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Collecting ftfy (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 23.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
            "Collecting pytorch-transformers==1.1.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.5)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.1.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.13.2)\n",
            "Requirement already satisfied: Six in /usr/local/lib/python3.6/dist-packages (from flask-cors>=3.0.7->allennlp) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.6.16)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (41.2.0)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.15.6)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.8.19)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.12.224)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.83)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.9.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Building wheels for collected packages: jsonnet, overrides, word2number, parsimonious, numpydoc, ftfy\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320318 sha256=8f03bd64811dcfe1d438050f3919c668c19118f3d81b70adab5a6ad6df7a5dce\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-1.9-cp36-none-any.whl size=4214 sha256=b7383d8fc3d5844c8e83714027a5c98952d2effaf5fee4ba5b71435071a8a716\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=07178384129391b59fadedea32128816dbc7386924584088782530da5a5afee0\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=c40f9024f9878b014ade80d5295c0547030177e8ab43494e5553752344359ab2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.1-cp36-none-any.whl size=31872 sha256=c75478152aaa3399b844b407e976359687820d838e6bc385e733d1205046b8f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=5deac7cf0966502382f4f85a9bc9402b5ceee73bab9c077ecd8ae0e522716739\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
            "Successfully built jsonnet overrides word2number parsimonious numpydoc ftfy\n",
            "Installing collected packages: jsonnet, flaky, overrides, word2number, conllu, flask-cors, tensorboardX, parsimonious, jsonpickle, responses, numpydoc, pytorch-pretrained-bert, unidecode, ftfy, pytorch-transformers, allennlp\n",
            "  Found existing installation: pytorch-transformers 1.2.0\n",
            "    Uninstalling pytorch-transformers-1.2.0:\n",
            "      Successfully uninstalled pytorch-transformers-1.2.0\n",
            "Successfully installed allennlp-0.8.5 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-1.9 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.6 tensorboardX-1.8 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pytorch_transformers"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2pS55YrNcgj",
        "colab_type": "code",
        "outputId": "02aa5ab1-f8df-4041-a20f-8273b1aa89da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings, BertEmbeddings, ELMoEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
        "word_embeddings = [WordEmbeddings('glove'), ELMoEmbeddings(), FlairEmbeddings('news-backward-fast')]\n",
        "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
        "\n",
        "\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "trainer.train('./', max_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:06:36,779 Reading data from .\n",
            "2019-09-25 03:06:36,780 Train: train.csv\n",
            "2019-09-25 03:06:36,781 Dev: dev.csv\n",
            "2019-09-25 03:06:36,784 Test: test.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "100%|██████████| 336/336 [00:00<00:00, 474666.94B/s]\n",
            "100%|██████████| 374434792/374434792 [00:07<00:00, 51693678.58B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:07:06,373 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "100%|██████████| 4361/4361 [00:00<00:00, 237054.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:07:06,398 [b'HUM', b'LOC', b'NUM', b'ABBR', b'ENTY', b'DESC']\n",
            "2019-09-25 03:07:06,466 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:07:06,467 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentLSTMEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): WordEmbeddings('glove')\n",
            "      (list_embedding_1): ELMoEmbeddings(model=1-elmo-original)\n",
            "      (list_embedding_2): FlairEmbeddings(\n",
            "        (lm): LanguageModel(\n",
            "          (drop): Dropout(p=0.25)\n",
            "          (encoder): Embedding(275, 100)\n",
            "          (rnn): LSTM(100, 1024)\n",
            "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=4196, out_features=256, bias=True)\n",
            "    (rnn): GRU(256, 512)\n",
            "    (dropout): Dropout(p=0.5)\n",
            "  )\n",
            "  (decoder): Linear(in_features=512, out_features=6, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            ")\"\n",
            "2019-09-25 03:07:06,468 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:07:06,469 Corpus: \"Corpus: 4361 train + 546 dev + 545 test sentences\"\n",
            "2019-09-25 03:07:06,471 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:07:06,472 Parameters:\n",
            "2019-09-25 03:07:06,473  - learning_rate: \"0.1\"\n",
            "2019-09-25 03:07:06,476  - mini_batch_size: \"32\"\n",
            "2019-09-25 03:07:06,478  - patience: \"3\"\n",
            "2019-09-25 03:07:06,480  - anneal_factor: \"0.5\"\n",
            "2019-09-25 03:07:06,482  - max_epochs: \"10\"\n",
            "2019-09-25 03:07:06,484  - shuffle: \"True\"\n",
            "2019-09-25 03:07:06,486  - train_with_dev: \"False\"\n",
            "2019-09-25 03:07:06,488  - batch_growth_annealing: \"False\"\n",
            "2019-09-25 03:07:06,490 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:07:06,492 Model training base path: \".\"\n",
            "2019-09-25 03:07:06,494 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:07:06,496 Device: cpu\n",
            "2019-09-25 03:07:06,498 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:07:06,501 Embeddings storage mode: cpu\n",
            "2019-09-25 03:07:06,506 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:07:12,068 epoch 1 - iter 0/137 - loss 1.91484761 - samples/sec: 74.84\n",
            "2019-09-25 03:08:12,691 epoch 1 - iter 13/137 - loss 2.08656985 - samples/sec: 6.86\n",
            "2019-09-25 03:09:13,455 epoch 1 - iter 26/137 - loss 1.98000769 - samples/sec: 6.85\n",
            "2019-09-25 03:10:15,857 epoch 1 - iter 39/137 - loss 1.91287484 - samples/sec: 6.67\n",
            "2019-09-25 03:11:11,251 epoch 1 - iter 52/137 - loss 1.79564144 - samples/sec: 7.51\n",
            "2019-09-25 03:12:10,509 epoch 1 - iter 65/137 - loss 1.74097147 - samples/sec: 7.02\n",
            "2019-09-25 03:13:07,229 epoch 1 - iter 78/137 - loss 1.68728253 - samples/sec: 7.34\n",
            "2019-09-25 03:14:04,707 epoch 1 - iter 91/137 - loss 1.63104457 - samples/sec: 7.24\n",
            "2019-09-25 03:14:58,317 epoch 1 - iter 104/137 - loss 1.56980777 - samples/sec: 7.76\n",
            "2019-09-25 03:15:55,117 epoch 1 - iter 117/137 - loss 1.52973626 - samples/sec: 7.33\n",
            "2019-09-25 03:16:52,279 epoch 1 - iter 130/137 - loss 1.49661636 - samples/sec: 7.28\n",
            "2019-09-25 03:17:14,696 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:17:14,698 EPOCH 1 done: loss 1.4834 - lr 0.1000\n",
            "2019-09-25 03:18:25,947 DEV : loss 1.2081245183944702 - score 0.5549\n",
            "2019-09-25 03:18:25,979 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 03:18:30,479 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:18:30,746 epoch 2 - iter 0/137 - loss 1.08518612 - samples/sec: 1595.58\n",
            "2019-09-25 03:18:34,490 epoch 2 - iter 13/137 - loss 1.09978162 - samples/sec: 111.50\n",
            "2019-09-25 03:18:38,148 epoch 2 - iter 26/137 - loss 1.04575223 - samples/sec: 114.15\n",
            "2019-09-25 03:18:41,415 epoch 2 - iter 39/137 - loss 0.98224073 - samples/sec: 127.89\n",
            "2019-09-25 03:18:44,789 epoch 2 - iter 52/137 - loss 0.98148503 - samples/sec: 123.82\n",
            "2019-09-25 03:18:47,979 epoch 2 - iter 65/137 - loss 0.95271556 - samples/sec: 130.98\n",
            "2019-09-25 03:18:51,418 epoch 2 - iter 78/137 - loss 0.93790766 - samples/sec: 121.44\n",
            "2019-09-25 03:18:54,685 epoch 2 - iter 91/137 - loss 0.95271629 - samples/sec: 127.85\n",
            "2019-09-25 03:18:58,242 epoch 2 - iter 104/137 - loss 0.93638939 - samples/sec: 117.56\n",
            "2019-09-25 03:19:01,747 epoch 2 - iter 117/137 - loss 0.92110246 - samples/sec: 119.20\n",
            "2019-09-25 03:19:05,508 epoch 2 - iter 130/137 - loss 0.90106861 - samples/sec: 111.14\n",
            "2019-09-25 03:19:07,109 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:19:07,111 EPOCH 2 done: loss 0.8925 - lr 0.1000\n",
            "2019-09-25 03:19:08,376 DEV : loss 0.8406743407249451 - score 0.6978\n",
            "2019-09-25 03:19:08,412 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 03:19:13,225 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:19:13,683 epoch 3 - iter 0/137 - loss 0.65630329 - samples/sec: 913.29\n",
            "2019-09-25 03:19:17,252 epoch 3 - iter 13/137 - loss 0.54614389 - samples/sec: 117.07\n",
            "2019-09-25 03:19:21,057 epoch 3 - iter 26/137 - loss 0.59077264 - samples/sec: 109.78\n",
            "2019-09-25 03:19:24,502 epoch 3 - iter 39/137 - loss 0.60101179 - samples/sec: 121.20\n",
            "2019-09-25 03:19:27,803 epoch 3 - iter 52/137 - loss 0.60673233 - samples/sec: 126.57\n",
            "2019-09-25 03:19:31,227 epoch 3 - iter 65/137 - loss 0.61348219 - samples/sec: 121.99\n",
            "2019-09-25 03:19:34,696 epoch 3 - iter 78/137 - loss 0.61644009 - samples/sec: 120.38\n",
            "2019-09-25 03:19:38,008 epoch 3 - iter 91/137 - loss 0.59695341 - samples/sec: 126.18\n",
            "2019-09-25 03:19:41,296 epoch 3 - iter 104/137 - loss 0.57222218 - samples/sec: 127.07\n",
            "2019-09-25 03:19:44,630 epoch 3 - iter 117/137 - loss 0.57399431 - samples/sec: 125.28\n",
            "2019-09-25 03:19:48,095 epoch 3 - iter 130/137 - loss 0.56686590 - samples/sec: 120.54\n",
            "2019-09-25 03:19:49,741 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:19:49,743 EPOCH 3 done: loss 0.5601 - lr 0.1000\n",
            "2019-09-25 03:19:51,021 DEV : loss 1.3467068672180176 - score 0.6172\n",
            "2019-09-25 03:19:51,056 BAD EPOCHS (no improvement): 1\n",
            "2019-09-25 03:19:51,058 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:19:51,345 epoch 4 - iter 0/137 - loss 0.87721348 - samples/sec: 1467.59\n",
            "2019-09-25 03:19:55,002 epoch 4 - iter 13/137 - loss 0.33262694 - samples/sec: 114.15\n",
            "2019-09-25 03:19:58,324 epoch 4 - iter 26/137 - loss 0.37793684 - samples/sec: 125.87\n",
            "2019-09-25 03:20:01,784 epoch 4 - iter 39/137 - loss 0.37023268 - samples/sec: 120.73\n",
            "2019-09-25 03:20:05,405 epoch 4 - iter 52/137 - loss 0.36404283 - samples/sec: 115.32\n",
            "2019-09-25 03:20:08,556 epoch 4 - iter 65/137 - loss 0.37338737 - samples/sec: 132.67\n",
            "2019-09-25 03:20:11,802 epoch 4 - iter 78/137 - loss 0.35698371 - samples/sec: 128.72\n",
            "2019-09-25 03:20:15,421 epoch 4 - iter 91/137 - loss 0.35527199 - samples/sec: 115.35\n",
            "2019-09-25 03:20:18,888 epoch 4 - iter 104/137 - loss 0.34901795 - samples/sec: 120.46\n",
            "2019-09-25 03:20:22,406 epoch 4 - iter 117/137 - loss 0.34908314 - samples/sec: 118.75\n",
            "2019-09-25 03:20:26,101 epoch 4 - iter 130/137 - loss 0.34443824 - samples/sec: 113.03\n",
            "2019-09-25 03:20:27,569 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:20:27,571 EPOCH 4 done: loss 0.3468 - lr 0.1000\n",
            "2019-09-25 03:20:28,822 DEV : loss 0.5290024280548096 - score 0.8388\n",
            "2019-09-25 03:20:28,864 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 03:20:33,288 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:20:33,584 epoch 5 - iter 0/137 - loss 0.33537939 - samples/sec: 1422.38\n",
            "2019-09-25 03:20:37,276 epoch 5 - iter 13/137 - loss 0.31312688 - samples/sec: 113.06\n",
            "2019-09-25 03:20:40,734 epoch 5 - iter 26/137 - loss 0.30698656 - samples/sec: 120.81\n",
            "2019-09-25 03:20:44,171 epoch 5 - iter 39/137 - loss 0.29937500 - samples/sec: 121.48\n",
            "2019-09-25 03:20:47,340 epoch 5 - iter 52/137 - loss 0.27652895 - samples/sec: 131.82\n",
            "2019-09-25 03:20:50,934 epoch 5 - iter 65/137 - loss 0.27452037 - samples/sec: 116.25\n",
            "2019-09-25 03:20:54,441 epoch 5 - iter 78/137 - loss 0.27410548 - samples/sec: 119.14\n",
            "2019-09-25 03:20:57,985 epoch 5 - iter 91/137 - loss 0.27283685 - samples/sec: 117.86\n",
            "2019-09-25 03:21:01,689 epoch 5 - iter 104/137 - loss 0.26211680 - samples/sec: 112.75\n",
            "2019-09-25 03:21:05,121 epoch 5 - iter 117/137 - loss 0.25973302 - samples/sec: 121.76\n",
            "2019-09-25 03:21:08,266 epoch 5 - iter 130/137 - loss 0.25524111 - samples/sec: 132.85\n",
            "2019-09-25 03:21:09,862 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:21:09,864 EPOCH 5 done: loss 0.2526 - lr 0.1000\n",
            "2019-09-25 03:21:11,061 DEV : loss 0.35976970195770264 - score 0.8974\n",
            "2019-09-25 03:21:11,094 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 03:21:15,645 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:21:15,974 epoch 6 - iter 0/137 - loss 0.24667531 - samples/sec: 1303.93\n",
            "2019-09-25 03:21:19,672 epoch 6 - iter 13/137 - loss 0.19749035 - samples/sec: 113.09\n",
            "2019-09-25 03:21:23,096 epoch 6 - iter 26/137 - loss 0.20326345 - samples/sec: 122.03\n",
            "2019-09-25 03:21:26,564 epoch 6 - iter 39/137 - loss 0.19799617 - samples/sec: 120.48\n",
            "2019-09-25 03:21:30,031 epoch 6 - iter 52/137 - loss 0.18485775 - samples/sec: 120.43\n",
            "2019-09-25 03:21:33,607 epoch 6 - iter 65/137 - loss 0.18653042 - samples/sec: 116.81\n",
            "2019-09-25 03:21:37,731 epoch 6 - iter 78/137 - loss 0.19114801 - samples/sec: 101.25\n",
            "2019-09-25 03:21:41,426 epoch 6 - iter 91/137 - loss 0.19215592 - samples/sec: 113.06\n",
            "2019-09-25 03:21:45,081 epoch 6 - iter 104/137 - loss 0.18857761 - samples/sec: 114.36\n",
            "2019-09-25 03:21:48,553 epoch 6 - iter 117/137 - loss 0.18988202 - samples/sec: 120.60\n",
            "2019-09-25 03:21:52,125 epoch 6 - iter 130/137 - loss 0.18859276 - samples/sec: 117.04\n",
            "2019-09-25 03:21:53,578 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:21:53,580 EPOCH 6 done: loss 0.1907 - lr 0.1000\n",
            "2019-09-25 03:21:54,781 DEV : loss 0.3355516493320465 - score 0.8974\n",
            "2019-09-25 03:21:54,812 BAD EPOCHS (no improvement): 1\n",
            "2019-09-25 03:21:59,526 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:21:59,765 epoch 7 - iter 0/137 - loss 0.15808134 - samples/sec: 1765.95\n",
            "2019-09-25 03:22:03,581 epoch 7 - iter 13/137 - loss 0.15902391 - samples/sec: 109.41\n",
            "2019-09-25 03:22:07,241 epoch 7 - iter 26/137 - loss 0.13419239 - samples/sec: 114.15\n",
            "2019-09-25 03:22:11,035 epoch 7 - iter 39/137 - loss 0.13986963 - samples/sec: 110.03\n",
            "2019-09-25 03:22:14,854 epoch 7 - iter 52/137 - loss 0.14691765 - samples/sec: 109.36\n",
            "2019-09-25 03:22:18,358 epoch 7 - iter 65/137 - loss 0.14315428 - samples/sec: 119.24\n",
            "2019-09-25 03:22:22,293 epoch 7 - iter 78/137 - loss 0.15147866 - samples/sec: 106.09\n",
            "2019-09-25 03:22:26,050 epoch 7 - iter 91/137 - loss 0.16061361 - samples/sec: 111.12\n",
            "2019-09-25 03:22:29,539 epoch 7 - iter 104/137 - loss 0.15612726 - samples/sec: 119.70\n",
            "2019-09-25 03:22:33,119 epoch 7 - iter 117/137 - loss 0.15223381 - samples/sec: 116.72\n",
            "2019-09-25 03:22:36,912 epoch 7 - iter 130/137 - loss 0.15380351 - samples/sec: 110.10\n",
            "2019-09-25 03:22:38,479 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:22:38,480 EPOCH 7 done: loss 0.1510 - lr 0.1000\n",
            "2019-09-25 03:22:39,661 DEV : loss 0.2985255718231201 - score 0.9139\n",
            "2019-09-25 03:22:39,694 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 03:22:44,484 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:22:44,749 epoch 8 - iter 0/137 - loss 0.01742846 - samples/sec: 1581.62\n",
            "2019-09-25 03:22:48,314 epoch 8 - iter 13/137 - loss 0.12496027 - samples/sec: 117.08\n",
            "2019-09-25 03:22:52,157 epoch 8 - iter 26/137 - loss 0.11681926 - samples/sec: 108.69\n",
            "2019-09-25 03:22:55,725 epoch 8 - iter 39/137 - loss 0.12257942 - samples/sec: 117.02\n",
            "2019-09-25 03:22:59,150 epoch 8 - iter 52/137 - loss 0.11178833 - samples/sec: 122.01\n",
            "2019-09-25 03:23:03,232 epoch 8 - iter 65/137 - loss 0.11589032 - samples/sec: 102.34\n",
            "2019-09-25 03:23:07,024 epoch 8 - iter 78/137 - loss 0.10964166 - samples/sec: 110.13\n",
            "2019-09-25 03:23:11,000 epoch 8 - iter 91/137 - loss 0.10803653 - samples/sec: 105.01\n",
            "2019-09-25 03:23:14,511 epoch 8 - iter 104/137 - loss 0.10462968 - samples/sec: 118.96\n",
            "2019-09-25 03:23:17,895 epoch 8 - iter 117/137 - loss 0.10987557 - samples/sec: 123.43\n",
            "2019-09-25 03:23:21,645 epoch 8 - iter 130/137 - loss 0.11146052 - samples/sec: 111.35\n",
            "2019-09-25 03:23:23,058 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:23:23,061 EPOCH 8 done: loss 0.1085 - lr 0.1000\n",
            "2019-09-25 03:23:24,332 DEV : loss 0.3592057228088379 - score 0.8974\n",
            "2019-09-25 03:23:24,369 BAD EPOCHS (no improvement): 1\n",
            "2019-09-25 03:23:24,371 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:23:24,645 epoch 9 - iter 0/137 - loss 0.05723068 - samples/sec: 1534.41\n",
            "2019-09-25 03:23:28,063 epoch 9 - iter 13/137 - loss 0.12404734 - samples/sec: 122.14\n",
            "2019-09-25 03:23:31,888 epoch 9 - iter 26/137 - loss 0.09993721 - samples/sec: 109.16\n",
            "2019-09-25 03:23:35,558 epoch 9 - iter 39/137 - loss 0.10883071 - samples/sec: 113.83\n",
            "2019-09-25 03:23:39,345 epoch 9 - iter 52/137 - loss 0.10131024 - samples/sec: 110.24\n",
            "2019-09-25 03:23:43,020 epoch 9 - iter 65/137 - loss 0.09455424 - samples/sec: 113.64\n",
            "2019-09-25 03:23:46,649 epoch 9 - iter 78/137 - loss 0.09388003 - samples/sec: 115.15\n",
            "2019-09-25 03:23:50,126 epoch 9 - iter 91/137 - loss 0.09373521 - samples/sec: 120.17\n",
            "2019-09-25 03:23:53,715 epoch 9 - iter 104/137 - loss 0.09521818 - samples/sec: 116.43\n",
            "2019-09-25 03:23:57,577 epoch 9 - iter 117/137 - loss 0.09701575 - samples/sec: 108.20\n",
            "2019-09-25 03:24:01,425 epoch 9 - iter 130/137 - loss 0.09564220 - samples/sec: 108.63\n",
            "2019-09-25 03:24:03,192 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:24:03,194 EPOCH 9 done: loss 0.0935 - lr 0.1000\n",
            "2019-09-25 03:24:04,472 DEV : loss 0.40976637601852417 - score 0.8938\n",
            "2019-09-25 03:24:04,505 BAD EPOCHS (no improvement): 2\n",
            "2019-09-25 03:24:04,506 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:24:04,756 epoch 10 - iter 0/137 - loss 0.04116422 - samples/sec: 1678.58\n",
            "2019-09-25 03:24:08,375 epoch 10 - iter 13/137 - loss 0.07333223 - samples/sec: 115.33\n",
            "2019-09-25 03:24:11,938 epoch 10 - iter 26/137 - loss 0.06110402 - samples/sec: 117.24\n",
            "2019-09-25 03:24:15,426 epoch 10 - iter 39/137 - loss 0.07078692 - samples/sec: 119.76\n",
            "2019-09-25 03:24:19,051 epoch 10 - iter 52/137 - loss 0.07459830 - samples/sec: 115.23\n",
            "2019-09-25 03:24:22,712 epoch 10 - iter 65/137 - loss 0.06937104 - samples/sec: 114.13\n",
            "2019-09-25 03:24:26,179 epoch 10 - iter 78/137 - loss 0.07545776 - samples/sec: 120.44\n",
            "2019-09-25 03:24:29,624 epoch 10 - iter 91/137 - loss 0.07727217 - samples/sec: 121.23\n",
            "2019-09-25 03:24:33,337 epoch 10 - iter 104/137 - loss 0.08014459 - samples/sec: 112.43\n",
            "2019-09-25 03:24:37,296 epoch 10 - iter 117/137 - loss 0.08151953 - samples/sec: 105.48\n",
            "2019-09-25 03:24:41,007 epoch 10 - iter 130/137 - loss 0.08046552 - samples/sec: 112.76\n",
            "2019-09-25 03:24:42,863 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:24:42,865 EPOCH 10 done: loss 0.0790 - lr 0.1000\n",
            "2019-09-25 03:24:44,139 DEV : loss 0.3186640739440918 - score 0.9249\n",
            "2019-09-25 03:24:44,180 BAD EPOCHS (no improvement): 0\n",
            "2019-09-25 03:24:53,926 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-25 03:24:53,942 Testing using best model ...\n",
            "2019-09-25 03:24:53,947 loading file best-model.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:574: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  result = unpickler.load()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:26:16,224 0.9266\t0.9266\t0.9266\n",
            "2019-09-25 03:26:16,226 \n",
            "MICRO_AVG: acc 0.8632 - f1-score 0.9266\n",
            "MACRO_AVG: acc 0.8701 - f1-score 0.9283833333333332\n",
            "ABBR       tp: 8 - fp: 1 - fn: 0 - tn: 536 - precision: 0.8889 - recall: 1.0000 - accuracy: 0.8889 - f1-score: 0.9412\n",
            "DESC       tp: 92 - fp: 2 - fn: 22 - tn: 429 - precision: 0.9787 - recall: 0.8070 - accuracy: 0.7931 - f1-score: 0.8846\n",
            "ENTY       tp: 89 - fp: 27 - fn: 6 - tn: 423 - precision: 0.7672 - recall: 0.9368 - accuracy: 0.7295 - f1-score: 0.8436\n",
            "HUM        tp: 120 - fp: 3 - fn: 8 - tn: 414 - precision: 0.9756 - recall: 0.9375 - accuracy: 0.9160 - f1-score: 0.9562\n",
            "LOC        tp: 94 - fp: 5 - fn: 3 - tn: 443 - precision: 0.9495 - recall: 0.9691 - accuracy: 0.9216 - f1-score: 0.9592\n",
            "NUM        tp: 102 - fp: 2 - fn: 1 - tn: 440 - precision: 0.9808 - recall: 0.9903 - accuracy: 0.9714 - f1-score: 0.9855\n",
            "2019-09-25 03:26:16,227 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.9266,\n",
              " 'dev_score_history': [0.5549,\n",
              "  0.6978,\n",
              "  0.6172,\n",
              "  0.8388,\n",
              "  0.8974,\n",
              "  0.8974,\n",
              "  0.9139,\n",
              "  0.8974,\n",
              "  0.8938,\n",
              "  0.9249],\n",
              " 'train_loss_history': [1.4833933989496997,\n",
              "  0.8925306229260717,\n",
              "  0.5600962136348668,\n",
              "  0.34675389033381954,\n",
              "  0.2526464889465022,\n",
              "  0.19069881340230468,\n",
              "  0.15098025739519266,\n",
              "  0.10854372590861834,\n",
              "  0.09353612496578781,\n",
              "  0.07901349325365231],\n",
              " 'dev_loss_history': [tensor(1.2081),\n",
              "  tensor(0.8407),\n",
              "  tensor(1.3467),\n",
              "  tensor(0.5290),\n",
              "  tensor(0.3598),\n",
              "  tensor(0.3356),\n",
              "  tensor(0.2985),\n",
              "  tensor(0.3592),\n",
              "  tensor(0.4098),\n",
              "  tensor(0.3187)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZQ1-b5WZZtG",
        "colab_type": "code",
        "outputId": "6b8114de-701f-430e-9f05-8a48c80129e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "classifier = TextClassifier.load('./best-model.pt')\n",
        "sentence = Sentence(\"How many flowers are there in this park?\")\n",
        "classifier.predict(sentence)\n",
        "print(sentence.labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-25 03:31:27,086 loading file ./best-model.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:574: DeprecationWarning: Call to deprecated class DocumentLSTMEmbeddings. (The functionality of this class is moved to 'DocumentRNNEmbeddings') -- Deprecated since version 0.4.\n",
            "  result = unpickler.load()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[NUM (0.9999963045120239)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R_dKz-YypoW",
        "colab_type": "code",
        "outputId": "4e2245e4-1f62-4d27-e567-fbca0c846b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from flair.data_fetcher import NLPTaskDataFetcher\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings, BertEmbeddings, ELMoEmbeddings, OpenAIGPTEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "corpus = NLPTaskDataFetcher.load_classification_corpus(Path('./'), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')\n",
        "word_embeddings = [OpenAIGPTEmbeddings()]\n",
        "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=1024, reproject_words=True, reproject_words_dimension=32)\n",
        "\n",
        "\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "trainer.train('./', max_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-24 11:53:36,816 Reading data from .\n",
            "2019-09-24 11:53:36,818 Train: train.csv\n",
            "2019-09-24 11:53:36,820 Dev: dev.csv\n",
            "2019-09-24 11:53:36,821 Test: test.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:452: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:457: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:464: DeprecationWarning: Call to deprecated function (or staticmethod) read_text_classification_file. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n",
            "100%|██████████| 815973/815973 [00:00<00:00, 5613426.57B/s]\n",
            "100%|██████████| 458495/458495 [00:00<00:00, 3796334.51B/s]\n",
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
            "100%|██████████| 273/273 [00:00<00:00, 59206.05B/s]\n",
            "100%|██████████| 478750579/478750579 [00:09<00:00, 51783555.13B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-24 11:53:55,035 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4361/4361 [00:00<00:00, 268142.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-24 11:53:55,057 [b'HUM', b'LOC', b'NUM', b'ABBR', b'ENTY', b'DESC']\n",
            "2019-09-24 11:53:55,062 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-24 11:53:55,064 Model: \"TextClassifier(\n",
            "  (document_embeddings): DocumentRNNEmbeddings(\n",
            "    (embeddings): StackedEmbeddings(\n",
            "      (list_embedding_0): OpenAIGPTEmbeddings(\n",
            "        model=0-openai-gpt\n",
            "        (model): OpenAIGPTModel(\n",
            "          (tokens_embed): Embedding(40478, 768)\n",
            "          (positions_embed): Embedding(512, 768)\n",
            "          (drop): Dropout(p=0.1)\n",
            "          (h): ModuleList(\n",
            "            (0): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (1): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (2): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (3): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (4): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (5): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (6): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (7): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (8): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (9): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (10): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "            (11): Block(\n",
            "              (attn): Attention(\n",
            "                (c_attn): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (attn_dropout): Dropout(p=0.1)\n",
            "                (resid_dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_1): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (c_fc): Conv1D()\n",
            "                (c_proj): Conv1D()\n",
            "                (dropout): Dropout(p=0.1)\n",
            "              )\n",
            "              (ln_2): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (word_reprojection_map): Linear(in_features=1536, out_features=32, bias=True)\n",
            "    (rnn): GRU(32, 1024, batch_first=True)\n",
            "    (dropout): Dropout(p=0.5)\n",
            "  )\n",
            "  (decoder): Linear(in_features=1024, out_features=6, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            ")\"\n",
            "2019-09-24 11:53:55,065 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-24 11:53:55,066 Corpus: \"Corpus: 4361 train + 546 dev + 545 test sentences\"\n",
            "2019-09-24 11:53:55,075 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-24 11:53:55,076 Parameters:\n",
            "2019-09-24 11:53:55,078  - learning_rate: \"0.1\"\n",
            "2019-09-24 11:53:55,079  - mini_batch_size: \"32\"\n",
            "2019-09-24 11:53:55,080  - patience: \"3\"\n",
            "2019-09-24 11:53:55,082  - anneal_factor: \"0.5\"\n",
            "2019-09-24 11:53:55,083  - max_epochs: \"10\"\n",
            "2019-09-24 11:53:55,086  - shuffle: \"True\"\n",
            "2019-09-24 11:53:55,087  - train_with_dev: \"False\"\n",
            "2019-09-24 11:53:55,088  - batch_growth_annealing: \"False\"\n",
            "2019-09-24 11:53:55,089 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-24 11:53:55,090 Model training base path: \".\"\n",
            "2019-09-24 11:53:55,091 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-24 11:53:55,092 Device: cpu\n",
            "2019-09-24 11:53:55,093 ----------------------------------------------------------------------------------------------------\n",
            "2019-09-24 11:53:55,094 Embeddings storage mode: cpu\n",
            "2019-09-24 11:53:55,096 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-09-24 11:53:59,673 epoch 1 - iter 0/137 - loss 1.80280530 - samples/sec: 90.95\n",
            "2019-09-24 11:54:54,549 epoch 1 - iter 13/137 - loss 1.72893476 - samples/sec: 7.59\n",
            "2019-09-24 11:55:56,126 epoch 1 - iter 26/137 - loss 1.68462484 - samples/sec: 6.76\n",
            "2019-09-24 11:56:56,275 epoch 1 - iter 39/137 - loss 1.67050674 - samples/sec: 6.92\n",
            "2019-09-24 11:57:51,298 epoch 1 - iter 52/137 - loss 1.64905939 - samples/sec: 7.57\n",
            "2019-09-24 11:58:53,562 epoch 1 - iter 65/137 - loss 1.63552886 - samples/sec: 6.69\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-da42592c6c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_label_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/trainers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, anneal_factor, patience, min_learning_rate, train_with_dev, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, batch_growth_annealing, shuffle, param_selection_mode, num_workers, sampler, use_amp, amp_opt_level, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                         \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/models/text_classification_model.py\u001b[0m in \u001b[0;36mforward_loss\u001b[0;34m(self, data_points)\u001b[0m\n\u001b[1;32m    113\u001b[0m     ) -> torch.tensor:\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/models/text_classification_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         text_embedding_list = [\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/embeddings.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0meverything_embedded\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/embeddings.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m   2680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;31m# embed words in the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0mlengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/embeddings.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/embeddings.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0meverything_embedded\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/embeddings.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0mpooling_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling_operation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m             \u001b[0muse_scalar_mix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_scalar_mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m         )\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/embeddings.py\u001b[0m in \u001b[0;36m_get_transformer_sentence_embeddings\u001b[0;34m(sentences, tokenizer, model, name, layers, pooling_operation, use_scalar_mix, bos_token, eos_token)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                     \u001b[0msubword_start_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m                     \u001b[0msubword_end_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen_subwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                     \u001b[0muse_scalar_mix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_scalar_mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m                 )\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/flair/embeddings.py\u001b[0m in \u001b[0;36m_extract_embeddings\u001b[0;34m(hidden_states, layers, pooling_operation, subword_start_idx, subword_end_idx, use_scalar_mix)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0mcurrent_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubword_start_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msubword_end_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m         \u001b[0mfirst_embedding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpooling_operation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"first_last\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mlast_embedding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for dimension 0 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gijwG-Uyz6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}